
<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Pengyu An's Homepage</title>

    <meta name="author" content="Pengyu An">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Pengyu An（安鹏宇）
                </p>
                <p>I am a first-year Ph.D. student in Computer Science at <a href="https://iiis.tsinghua.edu.cn/">Institute for Interdisciplinary Information Sciences (IIIS), Tsinghua University</a> under the supervision of Prof. <a href="https://hxu.rocks/">Huazhe Xu</a>. I obtained my bachelor's degree from <a href="https://en.sjtu.edu.cn/">Shanghai Jiao Tong University (SJTU)</a> with GPA ranking <b>1</b>/88 and won the <a href="https://www.jwc.sjtu.edu.cn/info/1030/115841.htm">Best Bachelor Thesis Award</a>. 
                </p>

                <p>
                My research interest mainly focuses on robotic manipulation, aiming to develop robots equipped with generalizable, versatile and robust manipulation capabilities. I am also interested in tactile sensing and human-robot interaction.
                </p>
                <p>
                During my undergraduate study, I am fortunate to be mentored by Prof. <a href="https://www.mvig.org/">Cewu Lu</a> and Prof. <a href="https://thinklab.sjtu.edu.cn/">Junchi Yan</a>. I am also a research visiting student/visitor at <a href="https://www.mit.edu/">Massachusetts Institute of Technology</a> and <a href="https://mitibmwatsonailab.mit.edu/">MIT-IBM Watson AI Lab</a> under the supervision of Prof. <a href="https://web.mit.edu/cocosci/josh.html">Josh Tenenbaum</a> and Prof. <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a>.
                </p>

                <!-- <p>
                  I am searching for part-time/full-time research internship opportunities. Feel free to reach out if you're interested in my research.
                </p> -->

                 
                <p style="text-align:center">
                  <a href="mailto:zg24@mails.tsinghua.edu.cn">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=ctFTmmgAAAAJ&hl">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/blakery-star">Github</a> &nbsp;/&nbsp;
                  <a href="images/Wechat.jpg">WeChat</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/Gu__Zhang">Twitter</a> &nbsp;/&nbsp;


                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <img style="width:100%;max-width:100%;object-fit: cover;" alt="profile photo" src="images/guzhang-graduate.jpg" class="hoverZoomLink">
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding-bottom: 0;"> <!-- 减少底部的padding -->
                  <h2 style="margin-bottom: 0px;">News</h2> <!-- 减少margin-bottom -->
                </td>
              </tr>
              <tr>
                <td>
                  <ul>
                    <li>[Sep. 2024]:Our paper <strong>Maniwhere </strong> is accepted by CoRL 2024.
                    <li>[Jul. 2024]:Our paper <strong>DiffTactile </strong> won <strong style="color: red;">best paper award</strong> in Noosphere workshop at RSS 2024.
                    <li>[Jul. 2024]:Our paper <strong>Robo-ABC </strong> is accepted by ECCV 2024.
                    <li>[May. 2024]:Our paper <strong>3D Diffusion Policy </strong> is accepted by RSS 2024.
                    <li>[Jan. 2024]:One paper <strong>ArrayBot</strong> is accepted by ICRA 2024.</li>
                    <li>[Jan. 2024]:Two papers are accepted by ICLR 2024, one is <strong>spotlight</strong>.</li>
                    <li>[Sep. 2023]:One paper is accepted by NeurIPS 2023.</li>
                    <li>[Jun. 2023]:One paper <strong>Flexible Handover</strong> is accepted by IROS 2023.</li>
                    <li>[Apr. 2023]:One paper <strong>IOT-CL</strong> is accepted by ICML 2023.</li>
                    
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
                <p>
                  <sup>*</sup> indicates equal contributions. Representative papers are highlighted.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
           

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/maniwhere.jpg" alt="clean-usnob" width="240" height="160" style="display:block;margin-left:auto;margin-right:auto;vertical-align:middle;">
              </td>
              <td width="75%" valign="middle">
                <span class="papertitle">Learning to Manipulate Anywhere: A Visual Generalizable Framework For Reinforcement Learning</span>
                <br>
                <a href="https://gemcollector.github.io/">Zhecheng Yuan<sup>*</sup></a>, 
                <a href="https://github.com/Stillwtm">Tianming Wei<sup>*</sup></a>, 
                <a href="https://huggingface.co/shuiqicheng">Shuiqi Cheng</a>, 
                <strong>Gu Zhang</strong>, 
                <a href="https://cypypccpy.github.io/">Yuanpei Chen</a>, 
                and <a href="http://hxu.rocks/">Huazhe Xu</a>
                <br>
                <em>Conference on Robot Learning(<strong>CoRL</strong>)</em>, 2024
                <br>
                <a href="https://gemcollector.github.io/maniwhere/">project page</a> /
                <a href="https://arxiv.org/abs/2407.15815">paper</a> /
                <a href="https://x.com/fancy_yzc/status/1816123322721264077">twitter</a>
                <p></p>
                <p>In this paper, we propose Maniwhere, a generalizable framework tailored for visual reinforcement learning, enabling the trained robot policies to generalize across a combination of multiple visual disturbance types.</p>
              </td>
            </tr>



           
            <tr style="background-color: #FFFFE0;">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/dp3.jpg" alt="clean-usnob" width="240" height="160" style="display:block;margin-left:auto;margin-right:auto;vertical-align:middle;">
              </td>
              <td width="75%" valign="middle">
                <span class="papertitle">3D Diffusion Policy: Generalizable Visuomotor Policy Learning via Simple 3D Representations</span>
                <br>
                <a href="https://yanjieze.com/">Yanjie Ze<sup>*</sup></a>, 
                <strong>Gu Zhang<sup>*</sup></strong>, 
                <a href="https://zkangning.github.io/">Kangning Zhang</a>,
                <a href="https://github.com/pummmmpkin">Chenyuan Hu</a>, 
                <a href="https://wang-muhan.github.io/">Muhan Wang</a>
                and <a href="http://hxu.rocks/">Huazhe Xu</a>
                <br>
                <em>Robotics: Science and Systems (<strong>RSS</strong>)</em>, 2024
                <br>
                <a href="https://3d-diffusion-policy.github.io/">project page</a> /
                <a href="https://3d-diffusion-policy.github.io/2024_arXiv_DP3.pdf">paper</a> /
                <a href="https://github.com/YanjieZe/3D-Diffusion-Policy">code</a> /
                <a href="https://twitter.com/ZeYanjie/status/1765414787775963232">twitter</a>
                <p></p>
                <p>In this paper, we present 3D Diffusion Policy (DP3), a novel visual imitation learning approach that incorporates the power of 3D visual representations
                  into diffusion policies, a class of conditional action generative
                  models.</p>
              </td>
            </tr>



            <tr style="background-color: #FFFFE0;">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/iclr.png" alt="clean-usnob" width="240" height="160" style="display:block;margin-left:auto;margin-right:auto;vertical-align:middle;">
              </td>
              <td width="75%" valign="middle">
                <span class="papertitle">DIFFTACTILE: A Physics-based Differentiable Tactile Simulator for Contact-rich Robotic Manipulation</span>
                <br>
                <a href="https://si-lynnn.github.io/">Zilin Si<sup>*</sup></a>, 
                <strong>Gu Zhang<sup>*</sup></strong>, 
                <a href="https://www.qingweiben.com/">Qingwei Ben<sup>*</sup></a>,
                <a href="https://www.csail.mit.edu/person/brandon-romero">Branden Romero</a>, 
                <a href="https://www.zhou-xian.com/">Xian Zhou</a>, 
                <a href="https://chaoliu.tech/">Chao Liu</a> 
                and <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a>
                <br>
                <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2024 
                <br>
                <em><strong style="color: red;">Best Paper Award</strong>, RSS Noosphere (Tactile Sensing) Worshop</em>, 2024
                <br>
                <a href="https://difftactile.github.io/">project page</a> /
                <a href="https://arxiv.org/pdf/2403.08716.pdf">paper</a> /
                <a href="https://github.com/Genesis-Embodied-AI/DiffTactile">code</a> /
                <a href="https://twitter.com/gan_chuang/status/1771268662353150183">twitter</a>
                <p></p>
                <p>In this paper, we introduce DIFFTACTILE, a physics-based and fully differentiable tactile simulation system designed to enhance robotic manipulation with dense and physically-accurate tactile feedback.

                </p>
              </td>
            </tr>


            
              

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/corl.jpeg" alt="clean-usnob" width="240" height="160">
                </td>
                <td width="75%" valign="middle">
                  <span class="papertitle">Thin-Shell Object Manipulations With Differentiable Physics Simulations</span>
                  <br>
                  <a href="https://wangyian-me.github.io/">Yian Wang<sup>*</sup></a>,
                  <a href="https://github.com/Alif-01">Juntian Zheng<sup>*</sup></a>, 
                  <a href="https://www.cnblogs.com/ACMLCZH">Zhehuan Chen</a>, 
                  <a href="https://www.zhou-xian.com/">Xian Zhou</a>, 
                  <strong>Gu Zhang</strong>, 
                  <a href="https://chaoliu.tech/">Chao Liu</a>, 
                  and <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a>
                  <br>
                  <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2024 <strong style="color: red;">[Spotlight]</strong>
                  <br>
                  <a href="https://vis-www.cs.umass.edu/ThinShellLab/">project page</a> /
                  <a href="https://openreview.net/pdf?id=KsUh8MMFKQ">paper</a> /
                  <a href="https://github.com/Genesis-Embodied-AI/ThinShellLab">code</a>

                  <p> In this paper, we introduce ThinShellLab, a fully differentiable simulation platform tailored for diverse thin-shell material interactions with varying properties.</p>
                  <p></p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/roboabc.jpg" alt="clean-usnob" width="240" height="160">
                </td>
                <td width="75%" valign="middle">
                  <span class="papertitle">Robo-ABC: Affordance Generalization Beyond Categories via Semantic Correspondence for Robot Manipulation</span>
                  <br>
                  <a href="https://scholar.google.com/citations?user=jOPXmhIAAAAJ&hl=zh-CN">Yuanchen Ju<sup>*</sup></a>,
                  <a href="https://hukz18.github.io/">Kaizhe Hu<sup>*</sup></a>, 
                  <a href="https://malcomf1.github.io/">Guowei Zhang</a>, 
                  <strong>Gu Zhang</strong>, 
                  Mingrun Jiang, 
                  and <a href="https://hxu.rocks/">Huazhe Xu</a>
                  <br>
                  <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2024
                  <br>
                  <a href="https://tea-lab.github.io/Robo-ABC/">project page</a> /
                  <a href="https://arxiv.org/abs/2401.07487">paper</a> /
                  <a href="https://github.com/TEA-Lab/Robo-ABC">code</a>
                  <p> In this paper, we present Robo-ABC, a framework through which robots can generalize to manipulate out-of-category objects in a zero-shot manner without any manual annotation, additional training, part segmentation, pre-coded knowledge, or viewpoint restrictions.</p>
                  <p></p>
                </td>
              </tr>

             


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/arraybot.jpeg" alt="clean-usnob" width="240" height="160">
                </td>
                <td width="75%" valign="middle">
                  <span class="papertitle">ArrayBot: Reinforcement Learning for Generalizable Distributed Manipulation through Touch</span>
                  <br>
                  <a href="https://steven-xzr.github.io/">Zhengrong Xue<sup>*</sup></a>,
                  <a href="https://openreview.net/profile?id=~Han_Zhang25">Han Zhang<sup>*</sup></a>, 
                  <a href="https://openreview.net/profile?id=~Jingwen_Cheng1">Jingwen Chen</a>, 
                  <a href="https://zhengmaohe.netlify.app/">Zhengmao He</a>, 
                  <a href=https://scholar.google.com/citations?user=jOPXmhIAAAAJ&hl=zh-CN&oi=ao">Yuanchen Ju</a>, 
                  <a href="https://linchangyi1.github.io/">Changyi Lin</a>, 
                  <strong>Gu Zhang</strong>, 
                  and <a href="https://hxu.rocks/">Huazhe Xu</a>
                  <br>
                  <em>International Conference on Robotics and Automation(<strong>ICRA</strong>)</em>, 2024
                  <br>
                  <a href="https://steven-xzr.github.io/ArrayBot/">project page</a> /
                  <a href="https://arxiv.org/pdf/2306.16857">paper</a> /
                  <a href="https://github.com/Steven-xzr/ArrayBot">code</a>
                  <p> In this paper, we present ArrayBot, a distributed manipulation system consisting of a 16×16 array of vertically sliding pillars integrated with tactile sensors, which can simultaneously support, perceive, and manipulate the tabletop objects.</p>
                  <p></p>
                </td>
              </tr>
              
              <tr style="background-color: #FFFFE0;"> 
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/iros2023.png" alt="clean-usnob" width="240" height="160" style="display:block;margin-left:auto;margin-right:auto;vertical-align:middle;">
                </td>
                <td width="75%" valign="middle">
                  <span class="papertitle">Flexible Handover with Real-Time Robust Dynamic Grasp Trajectory Generation</span>
                  <br>
                  <strong>Gu Zhang</strong>, <a href="https://fang-haoshu.github.io/">Hao-shu Fang</a>, <a href="https://tonyfang.net/">Hongjie Fang</a> and <a href="https://www.mvig.org/">Cewu Lu</a>
                  <br>
                  <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (<strong>IROS</strong>)</em>, 2023 <strong style="color: red;">[Oral]</strong>
                  <br>
                  <a href="https://arxiv.org/abs/2308.15622">paper</a> 
                  <p></p>
                  <p>In this paper, we propose an approach for effective and robust flexible handover, which enables the robot to grasp moving objects with flexible motion trajectories with a high success rate.</p>
                </td>
              </tr>
              


            

            
            
            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/icml2023.jpeg" alt="clean-usnob" width="240" height="160">
                </td>
                <td width="75%" valign="middle">
                  <span class="papertitle">Understanding and Generalizing Contrastive Learning from the Inverse Optimal Transport Perspective</span>
                  <br>
                  <a href="https://www.researchgate.net/profile/Liangliang-Shi-2">Liangliang Shi</a>,
                  <strong>Gu Zhang</strong>, 
                  <a href="https://haoyuzhen.com/">Haoyu Zhen</a>, 
                  and <a href="https://thinklab.sjtu.edu.cn/">Junchi Yan</a>
                  <br>
                  <em>International Conference on Machine Learning (<strong>ICML</strong>)</em>, 2023
                  <br>
                  <a href="https://openreview.net/pdf?id=DBlWCsOy94">paper</a> 
                  <p></p>
                  <p>In this paper, we aim to understand CL with a collective point set matching perspective and formulate CL as a form of inverse optimal transport (IOT).</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/nips.jpeg" alt="clean-usnob" width="240" height="160">
                </td>
                <td width="75%" valign="middle">
                  <span class="papertitle">Relative Entropic Optimal Transport: a (Prior-aware) Matching Perspective to (Unbalanced) Classification</span>
                  <br>
                  <a href="https://www.researchgate.net/profile/Liangliang-Shi-2">Liangliang Shi</a>,
                  <a href="https://haoyuzhen.com/">Haoyu Zhen</a>, 
                  <strong>Gu Zhang</strong>, 
                  and <a href="https://thinklab.sjtu.edu.cn/">Junchi Yan</a>
                  <br>
                  <em>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2023
                  <br>
                  <a href="https://openreview.net/pdf?id=l61Kp1zBwC">paper</a> 
                  <p></p>
                  <p>In this paper, we propose a new variant of optimal transport, called Relative Entropic Optimal Transport (RE-OT) and verify its effectiveness for inhancing visual learning.</p>
                </td>
              </tr>

              


              


            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                  <tr>
                    <td style="padding-bottom: 0;"> <!-- 减少底部的padding -->
                      <h2 style="margin-bottom: 0px;">Selected Awards and Honors</h2> <!-- 减少margin-bottom -->
                    </td>
                  </tr>
                  <tr>
                    <td>
                      <ul>
                        <li>2024: <strong>Best Paper Award in Noosphere workshop at RSS 2024</strong></li>
                        <li>2024: Best Bachelor Thesis Award of Shanghai Jiao Tong University</li>
                        <li>2024: Shanghai Outstanding Graduates</li>
                        <li>2024: Zhiyuan Outstanding Scholarship (Top 30 winnners in Zhiyuan Honor College, ¥20000 RMB)</li>
                        <li>2023: <strong>SenseTime Scholarship (Top30 undergraduate AI researchers nationwide, ¥20000 RMB)</strong></li>
                        <li>2023: Shanghai Scholarship (Top 0.2% Shanghai, ¥8000 RMB)</li>
                        <li>2022: <strong>National Scholarship (Top 0.2% nationwide, ¥8000 RMB)</strong></li>
                        <li>2022: Hanying Juhua Scholarship (Top 15 winners in Zhiyuan Honor College, ¥15000 RMB)</li>
                        <li>2021: <strong>National Scholarship (Top 0.2% nationwide, ¥8000 RMB)</strong></li>
                        <li>2021, 2022: A-level Merit Scholarship (Top 1% SJTU, ¥1500 RMB)</li>
                        <li>2021, 2022: Merit Student (Top 5% SJTU)</li>
                        <li>2020, 2021, 2022, 2023: Zhiyuan Honor Scholarship (Top 5% SJTU, ¥5000 RMB)</li>

                      </ul>
                    </td>
                  </tr>
                </tbody>
              </table>
           
              
              <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                  <tr>
                    <td style="padding-bottom: 0;"> <!-- 减少底部的padding -->
                      <h2 style="margin-bottom: -5px;">Service</h2> <!-- 减少margin-bottom -->
                    </td>
                  </tr>
                  <tr>
                    <td>
                      <ul>
                        <li>Reviewer: CoRL 2024, RA-L 2024, IJCAI 2024</li>
                      </ul>
                    </td>
                  </tr>
                </tbody>
              </table>
              
           

      
          
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Design and source code from <a href="https://jonbarron.info/">Jon Barron's website</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
